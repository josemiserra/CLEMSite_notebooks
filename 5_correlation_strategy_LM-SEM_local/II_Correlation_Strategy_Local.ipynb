{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Correlation Strategy Part II - Local\n",
    "\n",
    "  _This Jupyter Notebook belongs to the article : **CLEMSite, a software for automated phenotypic screens using light microscopy and FIB-SEM.** Please, cite the original paper if you make use of anything present in this notebook._\n",
    "  \n",
    "  In the experimental setup presented in the article, MatTek dishes were used to automatically correlate cells from light microscopy (LM) with electron microscopy (EM). In the Part I we developed a global transformation used to drive the microscope to the specific cell. In this step we will try to increase the accuracy going to the local region.\n",
    "  \n",
    "  \n",
    " ## Content:\n",
    "* [Why Local?](#why-local)\n",
    "* [Folders and data structure](#folders-and-data-structure)\n",
    "* [Sampling description](#sampling-description)\n",
    "* [Registration workflow](#registration-workflow)\n",
    "* [Results table](#results-table)\n",
    "* [Conclusions](#conclusions)\n",
    "* [How to improve accuracy](#accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Local? <a class=\"anchor\" id=\"why-local\"></a>\n",
    "\n",
    "<img src=\"diagram.png\">\n",
    "\n",
    " After determining the global transformation and the position of each cell in SEM stage coordinates, the SEM stage is moved to the precise position of the first target cell. This position will be the center of the region to be acquired.  Since the coincidence point method will build a mark in the surface, first we have to move 50 µm in x direction from the target, so we avoid the burning of the surface on top of the target cell by the mark. After restoring the original conditions (0 beam shift, focus and stigmation) the coincidence point calculation is performed.\n",
    " \n",
    "  If a map was created, the application will perform then a recalculation of the original position. First, it will extract a list of the closest landmarks to the target position. With this list in hand, it will apply the routine of landmark acquisition and determination again, for at least 4 landmarks. The default for the experiments was initially set to 6 landmarks and later updated to 8 landmarks (depends on the quality of the sample surface). In principle we could extend it to any N closest positions.  After the selection and re-detection of landmarks, a new affine transformation will be computed and the original position will be overwritten. \n",
    "\n",
    "There are several reasons of why this operation was performed:\n",
    "- If the sample was cracked in two or more parts or it had a deformed shape that prevent the surface to fit into a plane, a global transformation will not provide enough precision close to the target, increasing the error.\n",
    "- Samples, during operation and over time, move inside the microscope because of thermal drift and other factors (e.g. FIB/SEM lens losing calibration by usage). \n",
    "- If the original coincidence point is far away from the coincidence point of the current sample (which happens where a high tilt is present), the z value is altered in a big measure, which has an effect on the final position. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folders and data structure <a class=\"anchor\" id=\"folders-and-data-structure\"></a>\n",
    "\n",
    "The data gathered from the experiment contains many information and it is structured as it is shown here:\n",
    "```bash\n",
    "        ├───AUTOMATION_COMPLETE\n",
    "            ├───COPB1_Liquid\n",
    "                ├───14_November_2018_COPB1\n",
    "                │   ├───14112018_automation_no_spots_LM\n",
    "                │   │   ├───renamed\n",
    "                │   │   │    ├───prescan\n",
    "                │   │   │    │      ├───field--X01--Y16_0066\n",
    "                │   │   │    │      ...\n",
    "                │   │   │    ├───hr \n",
    "                │   │   │    ├───center_golgi.json\n",
    "                │   │   ...\n",
    "                │   ├───part1_SEM      \n",
    "                │   │   ├───SEM_project\n",
    "                │   │   │   ├───2L_field--X01--Y16_0023___0010\n",
    "                │   │   │   ....\n",
    "                │   │   ├───SEM_scans\n",
    "                │   │   ├───generated_coordinates_and_files\n",
    "        \n",
    "```  \n",
    "Each project has a name (14_November_2018_COPB1) and inside there is a folder for the LM, ended with \\_LM (all the light microscopy acquisition and processing) and one or several SEM folders ended with \\_SEM. There is one folder for each time there was a run in the FIB/SEM microscope, for example, part1_SEM is the first part, and part2_SEM corresponds to a second run.\n",
    "\n",
    "Inside the LM folder: \n",
    "   - **Original data**: we can find the original acquired data in the _4927_ folder, the processed images and features with cell profiler in the _4927--cp_, all the scripts used during the acquisition and the jupyter notebook for selection, where all the analysis and selection is stored (e.g. _AUTOMATION_notebook-14Nov.ipynb_ and _AUTOMATION\\_notebook-14Nov.html_).\n",
    "   - **Renamed data**: There is a folder called renamed, where all the preprocessing for landmark acquisition and validation of selection is done. The most important file is _selected\\_cells.csv_, where the selection of cells and its coordinates is stored. The files center_golgi.json and center_nuclei.json contain the image coordinates for the centroids of the respective organelles and cell nuclei. In addition to that, we find 2 subfolders:\n",
    "        - _prescan_ - contains the prescan images used by cell profiler, but now renamed in an easy way\n",
    "        - _hr_ - contains all the files acquired in the second round of the confocal: for each cell, the low resolution of the grid, dapi and golgi, and the high resolution scan with the stack.\n",
    "        \n",
    "      This two subfolders repeat the information of 4927 but they organize all the images in a way that CLEMSite can understand it. In CLEMSite_LM there is plugin that allows to do the renaming. For doing that, follow the instructions provided by the CLEMSite manual.    \n",
    "       The renamed data is structured according to a code that indicates the relation between the cell acquired in SEM and the cell acquired in LM. E.g. _X00--Y01_, corresponds to one specific position in the LM, and the acquisition in the SEM was named accordingly, in this case could be something like _3M_X00--Y01_01_. \n",
    "       \n",
    "Inside the SEM folder:\n",
    "   - **generated_coordinates_and_files** - Contains files with all the coordinates and project mappings.\n",
    "   - **SEM_scans** - Contains all the images and files generated during the scan of the grid surface\n",
    "   - **SEM_project**- Each folder contains the acquisition (the 3D volume) where:\n",
    "       - cross_ref folders are folders used to compute the crossing information locally\n",
    "       - intermediate files previous to the acquisition, e.g. autofocus on cross section and trench detection\n",
    "       - The folder ending with **\\_\\_acq** contains the acquisition data and all files generated during it (logs and tracking from RunChecker)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling description <a class=\"anchor\" id=\"sampling-description\"></a>\n",
    "   \n",
    "   From a total of 4 studies (**13JulySPOTS**, **19NovemberSPOTS**, **14NovemberCOPB1**, **21NovemberCOPB1**), we will select randomly 10 cells from each. The strategy would be to compare a manual registration with the computed one. Given the scales, it is likely that the manual registration error can hold around 2-3 micrometers errors (see later, \"Enhanced registration using local features\"), but currently we don't have a way to estimate the human error with experimental samples. Thus, we will assume the human registration as ground truth, that is, assuming we have enough local features identified visually to generate a precise overlay of light microscopy and SEM images. \n",
    "   \n",
    "   Since the surface of each SEM sample looks very different and the preservation of the grid for identification varies from one experiment to another, we will separate the results instead of providing a global RMSE error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration workflow\n",
    "\n",
    "_Note: do not confuse the registration between landmarks of LM and EM during the microscope running, with this registration workflow. In the first, we try to obtain the position, based on the N-closest nearest landmarks and at the end, that provides a position where the microscope starts to do its acquisition. The image taken in SEM is taken with this position at the center._\n",
    "\n",
    "We need to evaluate the RMSE of that position. For this reason, now we will register the LM image with the SEM image, and then, compute the distance from the organelle centroid to the center of the SEM image. The distance between them in microns will be used as RMSE. We do not use the transformation matrix used by the microscope, we evaluate independently, and be based only in local features of the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import os,re\n",
    "import sys\n",
    "\n",
    "# Auxiliary Function to get files by regular expression\n",
    "def filterPick(folder, myString):\n",
    "    myList = glob.glob(folder+'\\\\*')\n",
    "    pattern = re.compile(myString);\n",
    "    indices = [i for i, x in enumerate(myList) if pattern.search(x)]\n",
    "    return [ myList[ind] for ind in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\data\\\\19_November_SPOTS\\\\3K_field--X02--Y08_0025'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: define folders\n",
    "project_folder = \".\\\\data\\\\19_November_SPOTS\"\n",
    "project_sample = \"3K_field--X02--Y08_0025\"\n",
    "sample_path = os.path.join(project_folder,project_sample)\n",
    "sample_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find relevant folders\n",
    "LM_images_path = os.path.join(sample_path,\"field--X02--Y08_0025\")\n",
    "SEM_image_path = filterPick(sample_path,\"sFOV.*tif\")[0]  # Find the filename with sFOV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate LM position masks for the whole experiment:\n",
    "First, we are going to create masks that point the position of the cell in light microscopy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Masks \n",
    "import json\n",
    "\n",
    "with open(os.path.join(project_folder,\"masks//center_golgi.json\")) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "masks_folder = os.path.join(project_folder,\"masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "prescan_image_size = (680,680,3)\n",
    "# We are going to generate masks with the centroids for 680x680 images\n",
    "center_x = data['Location_Center_X'] # or   data['Mean_Golgi_AreaShape_Center_X']\n",
    "center_y = data['Location_Center_Y'] # or   data['Mean_Golgi_AreaShape_Center_Y']\n",
    "\n",
    "for key, cx in center_x.items():\n",
    "    cy = center_y[key]\n",
    "    im = np.zeros(prescan_image_size,dtype=np.uint8) # We know the prescan resolution was 680\n",
    "    coords = (int(np.round(cx)),int(np.round(cy)))\n",
    "    im = cv2.circle(im, coords , 2, (0, 255, 255), 2)\n",
    "    im[coords[1],coords[0],1] = 0\n",
    "    cv2.imwrite( masks_folder+'//'+key+'_.tif',im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extract SEM information\n",
    "\n",
    "The file used for registration is the one with the tag: _sFOV_. We can use the image _bFOV_ in case there is not enough visual landmarks to do our local registration. \n",
    "\n",
    "The information about pixel size can be read from the _.tif_ metadata using the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from tifftest import TiffFile,TiffWriter\n",
    "\n",
    "\n",
    "def getInfoHeaderAtlas(tifname):\n",
    "    xml_info = \"\"\n",
    "    data = {}\n",
    "    with TiffFile(tifname) as tif:\n",
    "        for page in tif:\n",
    "            for tag in page.tags.values():\n",
    "                if (tag.name == '51023' or tag.name =='fibics_xml'):\n",
    "                    xml_info = tag.value\n",
    "    if (not xml_info):\n",
    "        raise ValueError(\"NO INFO HEADER for ATLAS picture\")\n",
    "    root = ET.fromstring(xml_info)\n",
    "    first_tag = [];\n",
    "    second_tag = [];\n",
    "    third_tag = [];\n",
    "    for child in root:\n",
    "        m = re.match('Scan', child.tag)\n",
    "        m2 = re.match('.*Stage.*', child.tag)\n",
    "        m3 = re.match('.*Image$', child.tag)\n",
    "        if m:\n",
    "            first_tag = m.group(0)\n",
    "        elif m2:\n",
    "            second_tag = m2.group(0)\n",
    "        elif m3:\n",
    "            third_tag = m3.group(0)\n",
    "    #### Scan\n",
    "    if (first_tag):\n",
    "        child = root.findall(first_tag)\n",
    "        for el in child[0]:\n",
    "            if (el.tag == 'Ux'):\n",
    "                data['PixelSize'] = float(el.text)\n",
    "            elif (el.tag == 'Dwell'):\n",
    "                data['DwellTime'] = float(el.text)\n",
    "            elif (el.tag == 'FOV_X'):\n",
    "                data['FOV_X'] = float(el.text)\n",
    "            elif (el.tag == 'FOV_Y'):\n",
    "                data['FOV_Y'] = float(el.text)\n",
    "            elif (el.tag == 'Focus'):\n",
    "                data['WD'] = float(el.text)\n",
    "    ######## Stage\n",
    "    if (second_tag):\n",
    "        child = root.findall(second_tag)\n",
    "        for el in child[0]:\n",
    "            if (el.tag == 'X'):\n",
    "                data['PositionX'] = float(el.text)\n",
    "            elif (el.tag == 'Y'):\n",
    "                data['PositionY'] = float(el.text)\n",
    "            elif (el.tag == 'Z'):\n",
    "                data['PositionZ'] = float(el.text)\n",
    "    ######## Image\n",
    "    if (third_tag):\n",
    "        child = root.findall(third_tag)\n",
    "        for el in child[0]:\n",
    "            if (el.tag == 'Detector'):\n",
    "                data['Detector'] = el.text\n",
    "            elif (el.tag == 'Aperture'):\n",
    "                data['Aperture'] = el.text\n",
    "            elif (el.tag == 'Width'):\n",
    "                data['Width'] = int(el.text)\n",
    "            elif (el.tag == 'Height'):\n",
    "                data['Height'] = int(el.text)\n",
    "            elif (el.tag == 'Brightness'):\n",
    "                data['Brightness'] = float(el.text)\n",
    "            elif (el.tag == 'Contrast'):\n",
    "                data['Contrast'] = float(el.text)\n",
    "            elif (el.tag == 'Beam'):\n",
    "                data['Beam'] = el.text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DwellTime': 10000.0,\n",
       " 'FOV_X': 307.154449462891,\n",
       " 'FOV_Y': 307.154449462891,\n",
       " 'PixelSize': 0.299955517053604,\n",
       " 'WD': 0.00501363817602396,\n",
       " 'PositionX': -40007.2460528197,\n",
       " 'PositionY': -68152.6367931032,\n",
       " 'PositionZ': 41901.7124354794,\n",
       " 'Width': 1024,\n",
       " 'Height': 1024,\n",
       " 'Beam': 'SEM',\n",
       " 'Aperture': '1,5 kV | 700 pA [An]',\n",
       " 'Detector': 'SESI',\n",
       " 'Contrast': 19.9325046539307,\n",
       " 'Brightness': 51.5462379455566}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataAtlas = getInfoHeaderAtlas(SEM_image_path) # Get the header information\n",
    "dataAtlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Extract LM information\n",
    "Do the same for LM, extract pixel size information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfoTiffOME(tifname):\n",
    "    \"\"\" Get info from tiff header \"\"\"\n",
    "    pixel_size = 0\n",
    "    res_unit = 0\n",
    "    xml_info = []\n",
    "    with TiffFile(tifname) as tif:\n",
    "        for page in tif:\n",
    "            for tag in page.tags.values():\n",
    "                if (tag.name == 'image_description'):\n",
    "                    xml_info = tag.value\n",
    "                if (tag.name == 'resolution_unit'):\n",
    "                    res_unit = tag.value\n",
    "                if (tag.name == 'x_resolution'):  # we assume the same x and y resolution\n",
    "                    res_size = tag.value\n",
    "                if (tag.name == 'image_length'):\n",
    "                    im_length = tag.value\n",
    "    if (int(res_unit) == 3):  # dots per cm\n",
    "        tpx = float(res_size[0]) / float(res_size[1])  # pixels per 1 cm\n",
    "        pixelsize = 1.0 / tpx  # length of the image e.g. 1024 pixels/ tpx\n",
    "        pixel_size = pixelsize * 10  # change to meters\n",
    "\n",
    "    return (xml_info, pixel_size)\n",
    "\n",
    "def getInfoHeader(fname):\n",
    "    xml_info, pixel_size = getInfoTiffOME(fname)\n",
    "    if(not xml_info):\n",
    "        return\n",
    "    try:\n",
    "        root = ET.fromstring(xml_info)\n",
    "    except ET.ParseError as err:\n",
    "        return\n",
    "\n",
    "    first_tag = []\n",
    "    second_tag = []\n",
    "    for child in root:\n",
    "        m = re.match('.*Image.*', child.tag)\n",
    "        if m:\n",
    "            first_tag = m.group(0)\n",
    "    if (first_tag):\n",
    "        data = {}\n",
    "        for child in root.findall(first_tag):\n",
    "            for gch in child:\n",
    "                m = re.match('.*Pixels.*', gch.tag)\n",
    "                if m:\n",
    "                    second_tag = m.group(0)\n",
    "    if (second_tag):\n",
    "        child = root.findall(first_tag + '//' + second_tag)\n",
    "        for gch in child[0]:\n",
    "            planetag = re.match('.*Plane.*', gch.tag)\n",
    "        child2 = root.findall(first_tag + '//' + second_tag + '//' + planetag.group(0))\n",
    "        for gch in child2[0]:\n",
    "            stagepositiontag = re.match('.*StagePosition.*', gch.tag)\n",
    "        child2 = root.findall(\n",
    "            first_tag + '//' + second_tag + '//' + planetag.group(0) + '//' + stagepositiontag.group(0))\n",
    "        mydict = child2[0].attrib;\n",
    "        data['PositionX'] = float(mydict['PositionX']) * 1e6;\n",
    "        data['PositionY'] = float(mydict['PositionY']) * 1e6;\n",
    "        data['PositionZ'] = float(mydict['PositionZ']) * 1e6;\n",
    "        mydict = child[0].attrib;\n",
    "\n",
    "        data['PixelSize'] = pixel_size*1e3; # in micrometers!!  # float(mydict['PhysicalSizeX'])*1e-3;\n",
    "        #                data['PhysicalSizeY'] = mydict['PhysicalSizeY']\n",
    "        #                data['PhysicalSizeZ'] = mydict['PhysicalSizeZ']\n",
    "        data['PyxelType'] = mydict['PixelType']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PositionX': 57633.19803085,\n",
       " 'PositionY': 40164.59857634,\n",
       " 'PositionZ': -2.6609923,\n",
       " 'PixelSize': 0.37990264549510777,\n",
       " 'PyxelType': 'uint8'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescans = filterPick(LM_images_path,\"prescan.*tif\")\n",
    "dataLM = getInfoHeader(prescans[0])\n",
    "dataLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Auto adjust Brightness and Contrast of images\n",
    "Now, we prepare the images. First set up B&C from SEM image to observe better any salient features (like cells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preparation SEM\n",
    "# Auto brightness and contrast\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import exposure\n",
    "\n",
    "im =  cv2.imread(SEM_image_path,0)\n",
    "im_adapted = exposure.equalize_adapthist(im, clip_limit=0.005)\n",
    "im_adapted = im_adapted**2+0.012\n",
    "cv2.imwrite(SEM_image_path[:-20]+\"_adapted.tif\",im_adapted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the scale ratio between SEM and LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(861, 861)\n"
     ]
    }
   ],
   "source": [
    "from skimage import exposure\n",
    "ratio = dataLM['PixelSize']/dataAtlas['PixelSize']\n",
    "# 680x680 is the shape of the prescan\n",
    "new_shape = int(np.round(680*ratio)), int(np.round(680*ratio))\n",
    "print(new_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the open source software GIMP to do the manual registration.  We decided to go for a simple **rigid** transform which only allows one scaling, one horizontal flip and shift and rotations. By restraining the number of transformations and not allowing complete freedom, we expect to be less biased by not adapting the image to the data. \n",
    "\n",
    "Why GIMP? There are alternative softwares to do registration, like ecCLEM (http://icy.bioimageanalysis.org/plugin/ec-clem/) or several plugins for registration in FIJI. Using that software registration usually does an affine transform by clicking on landmarks over two images. However, the following procedure was really fast (one image can be aligned in 10-15 minutes).\n",
    "\n",
    "#### Instructions in GIMP to do a manual registration\n",
    "\n",
    "- Get the SEM image sFOV and drag it into GIMP 2.10\n",
    "- Add the mask image (if there are several masks corresponding to the code Xnn\\_\\_Ynn, look the mask that matches with reg\\_t.jpg)\n",
    "- Add the images corresponding to the prefix prescan (golgi, nuclei, transmitted light)\n",
    "- Create a layer group and add the mask and prescan images inside\n",
    "- Sort it: mask first, nuclei second, golgi third, transmitted light fourth and adjust transparencies with the bar on top.\n",
    "- Chain all images inside the layer group\n",
    "- Scale the layer group from 680x680 to 861x861 (Right Mouse click on image ->Scale Layer) \n",
    "- Adjust brightness and contrast of all the images:\n",
    "    - for the prescan images use Colors->Brightness & Contrast->Edit these settings as levels-> Auto Input levels\n",
    "- Flip Horizontally (Layer->Transform->Flip Horizontally)\n",
    "- Shift and rotate until:\n",
    "    - Imperfections and shapes in the transmitted light match with the SEM image \n",
    "    - Sometimes fluorescence staining (nuclei and golgi) matches to the contour visible cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " <img src=\"example.png\"  width=\"512\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results table for manual registration <a class=\"anchor\" id=\"results_table\"></a>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Sample</th>\n",
    "    <th>Value for Global Transformation</th>\n",
    "    <th>Value for Local Transformation (estimated)</th>\n",
    "    <th>Value for Local Transformation (experimental, n=10)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">13 Jul</td>\n",
    "    <td style=\"width:200px\">6.44 +/-4.3, n=53</td>\n",
    "    <td style=\"width:200px\"> 4.53 +/- 3.4</td>\n",
    "    <td style=\"width:200px\">4 +/- 1.85</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">19 Nov</td>\n",
    "    <td style=\"width:200px\">9.622 +/-5.1, n=46</td>\n",
    "    <td style=\"width:200px\">4.26 +/-3.09</td>\n",
    "    <td style=\"width:200px\">4.94 +/-3.5</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">14 Nov</td>\n",
    "    <td style=\"width:200px\">18.76 +/-11.5, n=33</td>\n",
    "    <td style=\"width:200px\">12.73 +/-10</td> \n",
    "    <td style=\"width:200px\">12 +/- 4.32 </td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">21 Nov</td>\n",
    "    <td style=\"width:200px\">20.56 +/-13.54, n=47</td>\n",
    "    <td style=\"width:200px\">14.98 +/-13.45</td>\n",
    "    <td style=\"width:200px\">9.86 +/- 6.47</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Average and SD</td>\n",
    "    <td style=\"width:200px\">13.21 +/-6.16, n=179</td>\n",
    "    <td style=\"width:200px\">8.71 +/- 5.17 , n=179</td>\n",
    "    <td style=\"width:200px\">7.7+/-4.36     , n=40</td>\n",
    "  </tr>\n",
    "    </table>\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a class=\"anchor\" id=\"results_table\"></a>\n",
    "\n",
    " We registered a total of 40 cells belonging to a 4 different datasets, sampling 10 cells per dataset. We used rigid registration using features in the SEM image of the surface (image taken just before acquisition, with the target in the center) and the LM using fluorescence. If available, we made use also of transmitted light. Rigid registration lacks of shearing, needed in many cases (the SEM surface was tilted), however, in order to save time, to simply overlay with shifts and rotations, basing the overlay in fitting the closest local features.\n",
    " \n",
    "  The datasets _14Nov_ and _21Nov_ displayed a big error, mostly caused by the errors in detection. By examining the SEM surface of both samples, it could be observed that the task of identifying accurately the landmarks was not a simple one. In case of the _14Nov_ sample the marks where too faint and covered with paint. In the case of _21Nov_ , the sample was cracked into several pieces:\n",
    " <img src=\"errors.png\">\n",
    "\n",
    " It is important to notice that we were able to predict accurately the targeting error before the acquisition. That can help us to decide if the error is too big so the sample must be discarded, or if we continue the acquisitions but we enlarge the milling area and FOV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve accuracy <a class=\"anchor\" id=\"ccuracy\"></a>\n",
    "\n",
    "#### Reduce distance between landmarks\n",
    " There is a constraint imposed by MatTek dishes: a square in the grid has a side of 600 µm, which means that for acquiring just 20 landmarks, the area to be covered is around 2,4 mm in diameter (√20 ≈ 4 landmarks in average). Scanning for landmarks in such a big extension is against the benefits explained in previous points (thermal and mechanical drift, avoid local areas with deformations). With other grids (like sapphire grids, ibidi or even with self made glass coverslips)  where the distance between landmarks is reduced, we could obtain more landmarks in a closer range, thus increasing the accuracy of the local transform. That would require, of course, an adaptation of the detection software.\n",
    "\n",
    "<img src=\"other_grids.png\">\n",
    "\n",
    "\n",
    "#### Enhanced registration by using local area features\n",
    " Using predefined landmarks has the downside that landmarks can be very far from the target. Experienced CLEM users tend to use local features present in both SEM and LM images to do an overlay (what we did in the manual registration procedure). Can we automate this procedure? \n",
    " A suggested way would be to find the contours of cells at high kV (5 or 10 kV) in SEM, and then use the contours detected using transmitted light or fluorescence. Overlaying the contours can provide then a high accuracy targeting precision.\n",
    "\n",
    "<img src=\"advanced_registration.png\">\n",
    "\n",
    "We tested this with 5 images using ITK for registration (by minimizing mutual information and restricted to an affine transformation). 3 of the 5 images were automatically registered. In the others, the optimization of the registration was not able to converge, providing a wrong alignment of both images.\n",
    "\n",
    "**Advantages**:\n",
    "   - In theory is possible to achieve the maximum accuracy, 2-3 micrometers precision proven or even less using other features as landmarks, like beads.\n",
    " \n",
    "**Disadvantages**:\n",
    "   - Depending on local features, does not guarantee the presence or successful detection of that features in the region where a cell is located.\n",
    "   - The appeareance of cells in the sample surface for SEM is very dependent on sample preparation.\n",
    "   - Might not work in very confluent configurations\n",
    "   - The current state of the art in registration requires to tinker around with parameters for each specific registration. That means, it is not easy to automate for light microscopy images acquired with different conditions. It requires to set up and handle a register procedure adapted to each experiment, e.g., which channels available, if the cell contours are well defined or not, etc..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('pyto': conda)",
   "language": "python",
   "name": "python37664bitpytoconda25fee6b740ef4077908199206548b669"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
